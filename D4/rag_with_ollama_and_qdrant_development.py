# -*- coding: utf-8 -*-
"""RAG with Ollama and QDrant Development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_CZgWmOdgte6Zc5s3RpZdkPB4l1n2EU
"""

# Download and install Ollama
!curl -fsSL https://ollama.com/install.sh | sh

# Install dependencies
# =========================================
!pip install -q streamlit qdrant-client langchain langchain-community pyngrok pypdf

# Pull the necessary models (e.g., nomic-embed-text and llama3.1:8b)
# Make sure these match the models specified in your app.py
!ollama pull nomic-embed-text
!ollama pull llama3

import subprocess, time
from pyngrok import ngrok


NGROK_AUTH_TOKEN = "NGROK_AUTH_TOKEN"  # <-- paste yours here if available
if NGROK_AUTH_TOKEN:
    ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Kill old tunnels if exist
ngrok.kill()

# Start Streamlit in background
port = 8501
cmd = f"streamlit run app.py --server.port {port} --server.address 0.0.0.0"
process = subprocess.Popen(cmd, shell=True)

# Wait for it to spin up
time.sleep(5)

# Connect ngrok tunnel
public_url = ngrok.connect(port)
print("âœ… Streamlit is running at:", public_url)

# Start the Ollama server in the background
# This will download the specified models if they are not already present
!ollama serve &

!streamlit run app.py --server.port 8501 --server.address 0.0.0.0

# Create app.py file and paste

import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.embeddings import OllamaEmbeddings
import qdrant_client
from qdrant_client.models import VectorParams, Distance


def run_app():
    st.sidebar.title("ðŸ“„ RAG with Ollama + Qdrant")
    st.title("ðŸ’¬ Document Chatbot (Colab)")

    # Qdrant connection settings
    qdrant_url = "qdrant_url"
    qdrant_api_key = "qdrant_api_key"

    # File uploader
    uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

    if uploaded_file and qdrant_url and qdrant_api_key:
        with open("uploaded.pdf", "wb") as f:
            f.write(uploaded_file.read())

        # Load and split
        loader = PyPDFLoader("uploaded.pdf")
        pages = loader.load_and_split()

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_documents(pages)

        # Init embeddings
        embeddings = OllamaEmbeddings(model="nomic-embed-text")

        # ðŸ”¹ Connect to Qdrant Cloud / Server with API key
        client = qdrant_client.QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key
        )

        # Always reset collection
        client.delete_collection(collection_name="docs")

        # ðŸ”¹ Create/reset collection
        client.recreate_collection(
            collection_name="docs",
            vectors_config=VectorParams(size=768, distance=Distance.COSINE),
        )

        # ðŸ”¹ Store vectors in Qdrant
        vectorstore = Qdrant.from_documents(
            chunks,
            embeddings,
            url=qdrant_url,
            api_key=qdrant_api_key,
            collection_name="docs",
        )

        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # Init LLM (Ollama)
        llm = Ollama(model="llama3")

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            return_source_documents=True
        )

        st.success("âœ… Document ingested successfully. Ask questions below!")

        query = st.text_area("Enter your question here:")
        if st.button("Ask"):
            if query.strip():
                result = qa_chain(query)
                st.write("### Answer:")
                st.write(result["result"])

                with st.expander("ðŸ“š Sources"):
                    for doc in result["source_documents"]:
                        st.markdown(doc.page_content[:500] + "...")
            else:
                st.warning("Please enter a question.")


# ðŸ”¹ Only run inside Streamlit
if __name__ == "__main__":
    run_app()
